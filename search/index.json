[{"content":"学习的必要性 必须要学习\n","date":"2021-08-11T14:57:58+08:00","permalink":"https://cyasts.github.io/p/howtolearn/","title":"学习"},{"content":"基于Hadoop的电影流量分析 一、环境安装 主机规划 四台主机，系统CentOS7、内存：1G、硬盘：20G\n四台服务器分配的IP地址：10.0.0.10/21/22/23\n规划：20用作主节点用作hadoop-master，其它三台为数据节点21、22、23用作hadoop-salve1~3\nJDK使用1.8版本\nhadoop使用2.8.5版本、hive使用2.3.6版本\n先安装master节点，然后采用克隆的方式复制出3台slave节点。这样可以减少配置的工作量。\n安装配置master节点 CentOS7安装过程不叙述。\n1、配置网卡 vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes NAME=ens33 UUID=28c7be74-55f0-4164-9e03-37da81b8f5c3 DEVICE=ens33 ONBOOT=yes IPADDR=10.0.0.10 NETMASK=255.255.255.0 GATEWAY=10.0.0.2 DNS1=8.8.8.8 2、修改主机名 vi /etc/hostname HOSTNAME=hadoop-master 3、安装软件 yum install -y lrzsz vim 4、配置hosts vim /etc/hosts 然后按照规划的主机和ip地址配置\n10.0.0.10 hadoop-master 10.0.0.21 hadoop-slave1 10.0.0.22 hadoop-slave2 10.0.0.23 hadoop-slave3 5、关闭防火墙和SELINUX 关闭防火墙 systemctl stop firewalld systemctl disable firewalld 关闭SELINUX vim /etc/selinux/config 修改其中的selinux为\nSELINUX=disabled 7、安装jdk 执行\nrz -y 上传文件，然后解压\ntar -zxvf jdk-8u291-linux-x64.tar.gz -C /usrl/local/ mv /usr/local/jdk-8u291-linux-x64 jdk 设置环境变量\nexport JAVA_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 8、安装hadoop 执行\nrz -y 上传文件，然后解压\ntar –zxvf hadoop-2.8.5.tar.gz –C /usr/local/ mv /usr/local/hadoop-2.8.5 /usr/local/hadoop 设置环境变量\nexport HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin 9、配置hadoop 1)配置core-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/usr/local/hadoop/tmp\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Abase for other temporary directories.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop-master:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.root.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.root.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; ~ 2)配置hdfs-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/hadoop/hdfs/name\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/hadoop/hdfs/data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.secondary.http-address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop-slave1:50090\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 3)配置mapred-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;http://hadoop-master:9001\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 4)配置yarn-site.xml \u0026lt;configuration\u0026gt; \u0026lt;!-- Site specific YARN configuration properties --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop-master\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 5)配置masters文件 修改/usr/local/hadoop/etc/hadoop/masters文件，该文件指定namenode节点所在的服务器机器。删除localhost，添加namenode节点的主机名hadoop-master；不建议使用IP地址，因为IP地址可能会变化，但是主机名一般不会变化。\nhadoop-master 6)配置slaves文件（Master主机特有） 修改/usr/local/hadoop/etc/hadoop/slaves文件，该文件指定哪些服务器节点是datanode节点。删除locahost，添加所有datanode节点的主机名.\nhadoop-slave1 hadoop-slave2 hadoop-slave3 克隆slave主机 在vmware虚拟机中对master主机进行克隆，要注意的是 这里一定要选完整克隆。克隆出三台并依次命名为\nhadoop-slave1 hadoop-slave2 最终主机如下：\n由于slave1~3主机为datanode，所以hadoop的配置文件中不能有slave文件。 依次对三台主机执行命令：\nrm -f /usr/local/hadoop/etc/hadoop/slaves 删除掉slaves文件\n然后修改hadoop-slave1~3的hostname和网卡ip地址和主机规划中的一样即可。\n集群配置 1、免密登录 1）.生成密钥 在每台主机上执行\nssh-keygen -t rsa 2）.分发密钥 master主机要和三台slave主机通信，所以要把master主机的密钥分发给全部的slave主机。\n在master主机上执行\nssh-copy-id -i .ssh/id_rsa.pub root@10.0.0.21 ssh-copy-id -i .ssh/id_rsa.pub root@10.0.0.22 ssh-copy-id -i .ssh/id_rsa.pub root@10.0.0.23 而slave1~3主机只需要和master主机通信，所以三台slave主机只需要分发密钥给master主机 在三台slave主机上分别执行\nssh-copy-id -i .ssh/id_rsa.pub root@10.0.0.10 3）.测试连接 在master主机上执行\nssh hadoop-slave 出现\n则说明免密登录成功。\n然后分别测试master和其他主机的连接。\n2、格式化hdfs和启动 执行\ncd /usr/local/hadoop/bin hdfs namenode -format 出现\n格式化成功。\n然后运行\ncd /usr/local/hadoop/sbin ./start-all.sh 出现\n启动成功。\n进入网页查看： http://10.0.0.10:50070\nhttp://10.0.0.10:8088\n3、在master主机上安装mariadb 1）.安装mariadb 执行\nyum -y install mariadb-server mariadb 然后执行\nmysql_secure_installation 设置root密码。\n2）.配置 然后修改mariadb的端口和字符集\nvim /etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock port=3306 # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 init_connect='SET collation_connection = utf8_unicode_ci' init_connect='SET NAMES utf8' character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/my.cnf.d 4、在master主机上安装hive 1）.安装hive 执行\nrz -y 上传文件\n然后解压文件\ntar –xzvf apache-hive-2.3.6-bin.tar.gz –C /usr/local/ mv /usr/local/apache-hive-2.3.6 /usr/local/hive 配置环境变量\nvim /etc/profile export HIVE_HOME=/usr/local/hive export HIVE_CONF_DIR=$HIVE_HOME/conf export PATH=$HIVE_HOME/bin:$PATH 2）.配置hive 配置文件位于/usr/local/hive/conf\ncd /usr/local/hive/conf 先复制hive-default.xml.template为hive-site.xml\ncp hive-default.xml.template hive-site.xml 然后编辑hive-site.xml,修改要连接的数据库和用户及密码。\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://10.0.0.10:3306/metastore?createDatabaseIfNotExist=true\u0026amp;amp;characterEncoding=utf-8\u0026amp;amp;useSSL=false\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;the URL of the MySQL database\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.cj.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Driver class name for a JDBC metastore\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;root\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/hive/warehouse\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exec.scratchdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/hive/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.querylog.location\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/hive/log\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; ~ 编辑 hive-env.sh，配置hadoop和jdk的路径\n# Set HADOOP_HOME to point to a specific hadoop install directory export HADOOP_HOME=/usr/local/hadoop # Hive Configuration Directory can be controlled by: export HIVE_CONF_DIR=/usr/local/hive/conf # Folder containing extra libraries required for hive compilation/execution can be controlled by: # export HIVE_AUX_JARS_PATH= export JAVA_HOME=/usr/local/jdk 3.将mysql的jdbc连接驱动上传至hive的lib目录 rz -y cp mysql-connector-java.jar /usr/local/hive/lib 4.在hdfs中创建hive-site.xml中配置的目录 hdfs dfs -mkdir /hive/ hdfs dfs -mkdir /hive/warehouse hdfs dfs -mkdir /hive/log hdfs dfs -mkdir /hive/tmp hdfs dfs -chmod -R 777 /hive 建立好的目录为:\n5.初始化hive元数据库 /usr/local/hive-2.1.1/bin/schematool -initSchema -dbType mysql 6.启动服务 nohup hive --service metastore \u0026amp; nohup hive --service hiveserver2 \u0026amp; 二、创建数据库 数据 数据来源于网络\n  users.dat\n数据格式为： 2::M::56::16::70072\n共有6040条数据\n对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String\n  movies.dat\n数据格式为： 2::Jumanji (1995)::Adventure|Children\u0026rsquo;s|Fantasy\n共有3883条数据\n对应字段为：MovieID BigInt, Title String, Genres String\n  ratings.dat\n数据格式为： 1::1193::5::978300760\n共有1000209条数据\n对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String\n  先将数据上传至/root/目录下\ncd /root/ rz -y 预处理 原始数据是以::进行切分的，hive不支持解析多字节的分隔符，也就是说hive只能解析':', 不支持解析'::'，所以用普通方式建表来使用是行不通的，要对数据做一次简单清洗。此处使用能解析多字节分隔符的Serde即可。使用RegexSerde需要两个参数：\ninput.regex = \u0026quot;(.*)::(.*)::(.*)\u0026quot; output.format.string = \u0026quot;%1$s %2$s %3$s\u0026quot; 创建数据库和表 1.规划 创建一个数据库movie，在movie数据库中创建3张表，t_user，t_movie，t_rating\nt_user:userid bigint,sex string,age int,occupation string,zipcode string t_movie:movieid bigint,moviename string,movietype string t_rating:userid bigint,movieid bigint,rate double,times string 2.创建数据库 create database if not exists movie; use movie; 3.创建t_user表并导入数据 create table t_user( userid bigint, sex string, age int, occupation string, zipcode string) row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties('input.regex'='(.*)::(.*)::(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s %4$s %5$s') stored as textfile; load data local inpath \u0026ldquo;/root/users.dat\u0026rdquo; into table t_user;\n4.创建t_movie表 create table t_movie( movieid bigint, moviename string, movietype string) row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties('input.regex'='(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s') stored as textfile; load data local inpath \u0026quot;/root/movies.dat\u0026quot; into table t_movie; 5.创建t_rating表 create table t_rating( userid bigint, movieid bigint, rate double, times string) row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties('input.regex'='(.*)::(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s %4$s') stored as textfile; load data local inpath \u0026quot;/root/ratings.dat\u0026quot; into table t_rating; 6.总表 创建好的表：\n三、统计分析 需求  根据用户观看的日期的时间分析在一周中哪一天观看电影的用户最多，也就是电影浏览量峰值。 统计被评分次数最多的10部电影，并给出评分次数（电影名，评分次数） 统计男性，女性当中评分最高的10部电影（性别，电影名，影评分） 统计评分\u0026gt;=4.0最多的那个年份的最好看的10部电影 各种类型电影中评价最高的5部电影（类型，电影名，平均影评分） 统计地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）  分析解决 1. 根据用户观看的日期的时间分析在一周中哪一天观看电影的用户最多，也就是电影浏览量峰值。 1）思路分析 由于rating.dat数据中的时间为unix时间戳，所以需要对时间进行转换\n编写python脚本进行数据清洗：对数据的日期进行转换\nimport sys import datetime for line in sys.stdin: line = line.strip() userid, movieid, rate, unixtime = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([userid, movieid, rate, str(weekday)]) 使用python脚本对原来的表中的数据进行清洗，然后再创建一个新的表，\nCREATE TABLE t_rating_new ( userid INT, movieid INT, rate INT, weekday INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'; add FILE /root/weekday_mapper.py; INSERT OVERWRITE TABLE t_rating_new SELECT TRANSFORM (userid, movieid, rate, times) USING 'python weekday_mapper.py' AS (userid, movieid, rate, weekday) FROM t_rating; 执行过程：\nmapreduce会自动计算任务工作量，然后分配节点数来完成任务。\n2）SQL 按照电影名进行分组统计，求出每部电影的评分次数并按照评分次数降序排序\nSELECT weekday, COUNT(1) cnt FROM t_rating_new GROUP BY weekday order by cnt desc; 3)结果 2. 统计被评分次数最多的10部电影，并给出评分次数（电影名，评分次数） 1）思路分析 按照电影名进行分组统计，求出每部电影的评分次数并按照评分次数降序排序\n2）SQL select a.moviename as moviename,count(a.moviename) as total from t_movie a join t_rating b on a.movieid=b.movieid group by a.moviename order by total desc limit 10; 3)结果 3. 分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分） 1）思路分析 t_user,t_movie,t_rating三表联合查询，按照性别过滤条件，电影名作为分组条件，影评分作为排序条件进行查询\n2）SQL  女性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次 select \u0026quot;F\u0026quot; as sex, c.moviename as name, avg(a.rate) as avgrate, count(c.moviename) as total from t_rating a join t_user b on a.userid=b.userid join t_movie c on a.movieid=c.movieid where b.sex=\u0026quot;F\u0026quot; group by c.moviename having total \u0026gt;= 50 order by avgrate desc limit 10;  男性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次 select \u0026quot;M\u0026quot; as sex, c.moviename as name, avg(a.rate) as avgrate, count(c.moviename) as total from t_rating a join t_user b on a.userid=b.userid join t_movie c on a.movieid=c.movieid where b.sex=\u0026quot;M\u0026quot; group by c.moviename having total \u0026gt;= 50 order by avgrate desc limit 10;   3)结果 4. 统计评分\u0026gt;=4.0最多的那个年份的最好看的10部电影 1）思路分析  由于电影名字后面有上映年份，所以可以将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来。 照年份进行分组条件，按照评分\u0026gt;=4.0作为where过滤条件，按照count(years)作为排序条件进行查询 按照第二步中查出的年份作为where过滤条件，按照评分作为排序条件进行查询  2）SQL   将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来\ncreate table rating_years as select a.movieid as movieid, a.moviename as moviename, substr(a.moviename,-5,4) as years, avg(b.rate) as avgrate from t_movie a join t_rating b on a.movieid=b.movieid group by a.movieid, a.moviename;   从rating_years按照年份进行分组条件，按照评分\u0026gt;=4.0作为where过滤条件，按照count(years)作为排序条件进行查询\nselect years, count(years) as total from rating_years a where avgrate \u0026gt;= 4.0 group by years order by total desc limit 1; 从结果中得到年份\n  where过滤条件，按照评分作为排序条件进行查找\nselect a.moviename as name, a.avgrate as rate from rating_years a where a.years=1998 order by rate desc limit 10;   3）结果 5. 各种类型电影中评价最高的5部电影（类型，电影名，平均影评分） 1）思路分析  需要电影类型，所以需要将moive中的type字段进行裂变，将结果保存 求TopN，按照type分组，需要添加一列来记录每组的顺序 从结果中出num列序号\u0026lt;=5的  2）SQL：  需要电影类型，所以要把t_movie表和rating_years连接，然后对type字段进行裂变 create table t_movie_sort as select c.id as id, c.name as name, c.years as years, c.rate as rate, tv.type as type from ( select b.movieid as id, b.moviename as name, b.years as years, b.avgrate as rate, a.movietype as type from t_movie a join rating_years b on a.movieid=b.movieid ) c lateral view explode(split(c.type,\u0026quot;\\\\|\u0026quot;)) tv as type;  按照电影类型和上映年份进行分组，按照影评分进行排序, 需要添加一列来记录每组的顺序。然后取出num列序号\u0026lt;=5 select * from ( select id,name,years,rate,type,row_number() over(distribute by type sort by rate desc) as num from t_movie_sort ) a where a.num \u0026lt;= 5;   3）结果 由于数据太多(电影总共有19中类型)，就只截部分图。\n6. 统计地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分） 1）思路分析  需要把t_movie，t_rating，t_user三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到 求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名 按照num=1作为where过滤条件取出结果数据  2）SQL   需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区\ncreate table rating_area as select c.movieid, c.moviename, avg(b.rate) as avgrate, a.zipcode from t_user a join t_rating b on a.userid=b.userid join t_movie c on b.movieid=c.movieid group by a.zipcode,c.movieid, c.moviename;   按照地区分组，按照平均评分排序，添加一列num用来记录地区排名，求TopN，然后按照num=1作为过滤条件取出结果数据\nselect t.* from ( select movieid,moviename,avgrate,zipcode, row_number() over (distribute by zipcode sort by avgrate) as num from rating_area ) t where t.num=1;   3）结果 同样由于数据太多(地区码太多)，就只截部分图。\n四、出现的问题 1. 问题 启动hadoop集群后，发现所有的datanode都没有成功启动\n在网页中查看存活的节点数也为0：\n2. 解决方法 网上查到：\n 当我们使用hadoop namenode -format格式化namenode时，会在namenode数据文件夹（这个文件夹为自己配置文件中dfs. name.dir的路径）中保存一个current/VERSION文件，记录clusterID，datanode中保存的current/VERSION文件中的clustreID的值是上一次格式化保存的clusterID，这样，datanode和namenode之间的ID不一致。\n 想到自己的hadoop环境经过了两次格式化，原因就是如此。要解决这个问题，只需要在dfs/name目录下找到一个current/VERSION文件，记录clusterID并复制。然后dfs/data目录下找到一个current/VERSION文件，将其中clustreID的值替换成刚刚复制的clusterID的值即可。\n问题成功解决。 ","date":"2021-06-25T19:41:36+08:00","permalink":"https://cyasts.github.io/p/hadoop/","title":"hadoop环境搭建"},{"content":"1.问题描述 构建成都地铁图，实现广度优先算法和 Dijkstra 算法\n2.输入 1.地铁图的输入 以线路号数开头，后面是该号线以此的站点数。每行一个数据。\n由于地铁中存在着环线，所以图的构建可能会成一个环。因此在输入的时候需要特别处理，把线路数乘以 10 再加上一个是否为环线，这样得到数字后取最后一位就知道是否为环线，再除以 10 就知道是几号线。例如 1 号线不是环线就变成 10，7 号线为环线就变成 71。\n最后以-1 为单独的一行，表示输入结束。 输入举例：\n10\n韦家碾\n火车北站\n人民北路\n\u0026hellip;\n广州路\n兴隆湖\n科学城\n20\n犀浦\n天河路\n百草路\n\u0026hellip;\n71\n火车北站\n\u0026hellip;\n-1\n2.测试站点的输入 输入两行，每行一个站点。\n如：\n石油大学\n理工大学\n3.输出 1.按照广度优先算法输出两个站的最短节点数路径。\n2.模拟两个站点之间的线路距离权重，输出两个站点之间的最短路径。\n4.数据结构 由于地铁图属于稀疏图，所以采用邻接链表来存储图。 graph.h 头文件：\n#ifndef GRAPH_H_INCLUDE #define GRAPH_H_INCLUDE  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; typedef struct edge{ int weight; int info; //线路名称 \tstruct edge * nextedge; struct vertex * adjtex; } edge; typedef struct vertex { char site[20]; struct edge * firstedge; struct vertex * prev; //用来存储结果路径上的上一个节点 \tint info; //用来存储多余的数据 \tint isdiscovered; } vertex; typedef struct { vertex ** vertices; int vexnum, edgnum; }graph; //function declare graph * buildgraph(); vertex * findvertexbyname(graph * g, const char * name); void cleartags(graph * g); vertex * findmininfo(graph * g); #endif graph.c 源文件：\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;graph.h\u0026#34; int buildedge(vertex * from, vertex * to, int lineno, int weight); int isexist(char line[300][20], const char * name, int k); //---------------------------------public function implements below ----------------------- graph * buildgraph() { char staname[20], line[300][20]; int i, k, l, weight; int lineno, iscycle; vertex * station[300], *v; vertex * pretex, * headtex; graph * metro; k = l = lineno = iscycle = 0; metro = (graph *)malloc(sizeof(graph)); metro-\u0026gt;vertices = NULL; metro-\u0026gt;vexnum = metro-\u0026gt;edgnum = 0; while(gets(staname) != NULL) { if(atoi(staname))//得到一条新的线路名称 \t{ if (iscycle) {//先处理上一条环线 \tweight = rand()%10 + 1; buildedge(pretex, headtex, lineno, weight); buildedge(headtex, pretex, lineno, weight); l += 2; } lineno = atoi(staname); if(lineno == -1) break; pretex = headtex = NULL; iscycle = lineno%10; lineno /= 10; } else { //查找是否站已经存在 \ti = isexist(line, staname, k); if (i \u0026gt; -1) v = station[i]; else { v = (vertex *)malloc(sizeof(vertex)); v-\u0026gt;firstedge = NULL; v-\u0026gt;isdiscovered = v-\u0026gt;info = 0; v-\u0026gt;prev = 0; station[k] = v; strcpy(line[k++], staname); strcpy(v-\u0026gt;site, staname); } if (pretex) { weight = rand()%10 + 1; buildedge(pretex, v, lineno, weight); buildedge(v, pretex, lineno, weight); l += 2; } if (!pretex \u0026amp;\u0026amp; iscycle) headtex = v; pretex = v; } } metro-\u0026gt;vexnum = k; metro-\u0026gt;edgnum = l; metro-\u0026gt;vertices = (vertex **)malloc(sizeof(vertex *) * k); for(i=0; i\u0026lt;k; ++i) metro-\u0026gt;vertices[i] = station[i]; return metro; } vertex * findvertexbyname(graph * g, const char * name) { int i; for(i=0; i\u0026lt;g-\u0026gt;vexnum; ++i) { if (strcmp(name, g-\u0026gt;vertices[i]-\u0026gt;site) ==0 ) return g-\u0026gt;vertices[i]; } } void cleartags(graph * g) { int i; for(i=0; i\u0026lt;g-\u0026gt;vexnum; ++i) { g-\u0026gt;vertices[i]-\u0026gt;isdiscovered = 0; g-\u0026gt;vertices[i]-\u0026gt;prev = 0; g-\u0026gt;vertices[i]-\u0026gt;info = 0; } } vertex * findmininfo(graph * g) { int i, mini, min = 999999; for (i=0; i\u0026lt;g-\u0026gt;vexnum; ++i) { if (g-\u0026gt;vertices[i]-\u0026gt;isdiscovered == 0) { if (g-\u0026gt;vertices[i]-\u0026gt;info \u0026lt; min) { mini = i; min = g-\u0026gt;vertices[i]-\u0026gt;info; } } } return g-\u0026gt;vertices[mini]; } //------------------------private function below here------------------------------ int isexist(char line[300][20], const char * name, int k) { int i; for(i=0; i\u0026lt;k; ++i) if(strcmp(name, line[i]) == 0) return i; return -1; } int buildedge(vertex * from, vertex * to, int lineno, int weight) { edge * arc = (edge *)malloc(sizeof(edge)); arc-\u0026gt;nextedge = from-\u0026gt;firstedge; from-\u0026gt;firstedge = arc; arc-\u0026gt;adjtex = to; arc-\u0026gt;info = lineno; arc-\u0026gt;weight = weight; } 4.BFS 函数 要输出从源站点到目标站点的路径，可利用节点中的 prev 指针存储父节点。这样到目标站点的时候沿着 prev 指针一直往上直到空结点，就得到了一个反向的路径，再利用栈，把路径上的结点全部入一次栈，最后依次出栈就得到了从源站点到目标站点的路径。具体代码如下：\nvoid BFS(graph * metro, vertex * src, vertex * des) { vertex * que[300]; int head, tail; head = tail = 0; vertex * u; edge * arc; que[tail++] = src; src-\u0026gt;isdiscovered = 1; while(tail != head) { u = que[head]; head = (head+1) % 300; if(u == des) break; arc = u-\u0026gt;firstedge; while (arc) { if (arc-\u0026gt;adjtex-\u0026gt;isdiscovered == 0) { arc-\u0026gt;adjtex-\u0026gt;prev = u; //构建一条下一层子节点到当前节点的边 \tque[tail] = arc-\u0026gt;adjtex; tail = (tail+1) % 300; arc-\u0026gt;adjtex-\u0026gt;isdiscovered = 1; } arc = arc-\u0026gt;nextedge; } } } 5.dijkstra 函数 同样的可利用站点的 prev 存储上一个站点，利用站点的 info 才存储该站点到源站点的最短距离。\nvoid dijkstra(graph * g, vertex * src) { int i, k, alt; vertex * u, *v; edge * arc; for(i=0; i\u0026lt;g-\u0026gt;vexnum; ++i) { g-\u0026gt;vertices[i]-\u0026gt;info = 99999; g-\u0026gt;vertices[i]-\u0026gt;prev = 0; } src-\u0026gt;info = 0; k = g-\u0026gt;vexnum; while(k \u0026gt; 0) { u = findmininfo(g); u-\u0026gt;isdiscovered = 1; k--; arc = u-\u0026gt;firstedge; while (arc) { v = arc-\u0026gt;adjtex; alt = u-\u0026gt;info + arc-\u0026gt;weight; if (alt \u0026lt; v-\u0026gt;info) { v-\u0026gt;info = alt; v-\u0026gt;prev = u; } arc = arc-\u0026gt;nextedge; } } } 5.主函数 printpath 的时候，由于 BFS 和 Dijkstra 都要调用这个函数，在最后设置一个 order 来区分是那个函数调用。\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026#34;graph.h\u0026#34; //function declare void dijkstra(graph * g, vertex * src); void BFS(graph * metro, vertex * src, vertex * des); void printpath(graph * metro, vertex * desc, int order); int main(int argc, char *argv[]) { freopen(\u0026#34;input.txt\u0026#34;, \u0026#34;r\u0026#34;, stdin); freopen(\u0026#34;output.txt\u0026#34;, \u0026#34;w\u0026#34;, stdout); graph * metro; vertex * src; vertex * des; metro = buildgraph(); char s1[20]; char s2[20]; scanf(\u0026#34;%s\u0026#34;, \u0026amp;s1); scanf(\u0026#34;%s\u0026#34;, \u0026amp;s2); src = findvertexbyname(metro, s1); des = findvertexbyname(metro, s2); printf(\u0026#34;in BFS:\\n\u0026#34;); BFS(metro, src, des); printpath(metro, des, 0); cleartags(metro); printf(\u0026#34;indjstl:\\n\u0026#34;); dijkstra(metro, src); printpath(metro, des, 1); fclose(stdin); fclose(stdout); return 0; } void printpath(graph * metro, vertex * desc, int order) { int i=0; vertex * sta[300]; vertex * v = desc; while(v) { sta[i++] = v; v = v-\u0026gt;prev; } for(i--; i\u0026gt;=0; --i){ if(order) printf(\u0026#34;%s %d\\n\u0026#34;, sta[i]-\u0026gt;site, sta[i]-\u0026gt;info); else printf(\u0026#34;%s \\n\u0026#34;, sta[i]-\u0026gt;site); } } 整体可于 https://github.com/cyasts/Algorithms/tree/master/2.BFS 获取。\n6.测试数据 1.输入 见：\nhttps://github.com/cyasts/Algorithms/blob/master/2.BFS/input.txt\n2.输出\nin BFS: 石油大学\n钟楼\n马超西路\n团结新区\n锦水河\n三河场\n金华寺东路\n植物园\n军区总医院\n熊猫大道\n动物园\n昭觉寺南路\n驷马桥\n府青路\n八里庄\n二仙桥\n理工大学\nin Dijkstra:\n石油大学 0\n钟楼 3\n马超西路 12\n团结新区 19\n锦水河 20\n三河场 23\n金华寺东路 28\n植物园 37\n军区总医院 44\n熊猫大道 50\n动物园 51\n昭觉寺南路 61\n驷马桥 62\n府青路 65\n八里庄 66\n二仙桥 68\n理工大学 75\n","date":"2020-10-26T22:38:36+08:00","permalink":"https://cyasts.github.io/p/graph/","title":"图的广度优先遍历和Dijkstra算法"},{"content":"鲁迅经典名言  希望是附丽于存在的，有存在，便有希望，有希望，便是光明。 哀其不幸，怒其不争。 悲剧将人生的有价值的东西毁灭给人看，喜剧将那无价值的撕破给人看。 伟大的心胸，应该表现出这样的气概——用笑脸来迎接悲惨的厄运，用百倍的勇气来应付一切的不幸。 不满足是向上的齿轮。 只看一个人的着作，结果是不大好的：你就得不到多方面的优点。必须如蜜蜂一样，采过许多花，这才能酿出蜜来。倘若叮在一处，所得就非常有限，枯燥了。 中国虽发明火药，却只会用来放烟花炮竹；发明罗盘，也是用来看风水。 渡尽劫波兄弟在，相逢一笑泯恩仇。 躲进小楼成一统，管它冬夏与春秋。 游戏是儿童最正当的行为，玩具是儿童的天使。 横眉冷对千夫指，俯首甘为孺子牛。 真正的勇士，敢于直面惨淡的人生，敢于正视淋漓的鲜血这是怎样的哀痛者和幸福者？ 改造自己，总比禁止别人来得难。 在行进时，也时时有人退伍，有人落荒，有人颓唐，有人叛变，然而只要无碍于进行，则越到后来，这队伍也就越成为纯粹. 精锐的队伍了。 单是说不行，要紧的是做。 巨大的建筑，总是由一木一石叠起来的，我们何妨做做这一木一石呢？我时常做些零碎事，就是为此。 空谈之类，是谈不久，也谈不出什麽来的，它始终被事实的镜子照出原形，拖出尾巴而去。 不满足是向上的车轮。 无情未必真豪杰，怜子如何不丈夫。 友谊是两颗心的真诚相待，而不是一颗心对另一颗心的敲打。 以人为鉴，明白非常，是使人能够反省的妙法。 父母对于子女，应该健全的产生，尽力的教育，完全的解放。 时间就像海绵里的水，只要愿挤，总还是有的。 倘只看书，便变成书橱。 我好像是一只牛，吃的是草，挤出的是奶。 其实地上本没有路，走的人多了，便成了路。 哪里有天才，我只是把别人喝咖啡的工夫都用在了工作上了。 唯有民魂是值得宝贵的，唯有它发扬起来，中国才有真进步。 沉着. 勇猛，有辨别，不自私。 愈艰难，就愈要做。改革，是向来没有一帆风顺的。 我们目下的当务之急是：一要生存，二要温饱，三要发展。 必须敢于正视，这才可望敢想. 敢说. 敢做. 敢当。 曾经阔气的要复古，正在阔气的要保持现状，未曾阔气的要革新，大抵如此，大抵！ 人类总不会寂寞，因为生命是进步的，是天生的。 只要从来如此，便是宝贝。 事实是毫无情面的东西，它能将空言打得粉碎。 墨写的谎说，决掩不住血写的事实。 其实先驱者本是容易变成绊脚石的。 贪安稳就没有自由，要自由就要历些危险。只有这两条路。 假使做事要面面顾到，那就什么事都不能做了。 时间就是性命。无端的空耗别人的时间，其实是无异于谋财害命的。 做一件事，无论大小，倘无恒心，是很不好的。 死者倘不埋在活人心中，那就真真死掉了。 改造自己，总比禁止别人来的难。 只要能培一朵花，就不妨做做会朽的腐草。 当我沉默的时候，我觉得充实；我将开口，同时感到空虚。 死亡的生命已经朽腐。我对于这朽腐有大欢喜，因为我借此知道它还非空虚。 但我坦然，欣然。我将大笑，我将歌唱。 我自爱我的野草，但我憎恶这以野草作装饰的地面。 待我成尘时，你将见我的微笑！  ","date":"2020-10-12T16:57:58+08:00","permalink":"https://cyasts.github.io/p/lx/","title":"鲁迅经典名言"},{"content":"1.问题描述 给定 N 个男性 $ m_{1} $ ,⋯,$m_{n}$ 和 N 个女性$w_{1}$,⋯,$w_{n}$\n 每个男性对女性由最喜欢到最不喜欢进行排序 每个女性对男性由最喜欢到最不喜欢进行排序  目标：找到一个稳定的匹配使得不满意度最低。\n2.输入 第一行输入 N，表示需要处理的匹配的对数。男性按照 1，2, 3\u0026hellip;N 的编号表示，女性同样如此。\n接下来 N 行，每行 N 个数字，表示该行男性对所有女性的好感度排名。\n再接下来 N 行，每行 N 个数字，表示该行女性对所有男性的好感度排名。\n例如：\n3 1 2 3 2 1 3 1 2 3 2 1 3 1 2 3 1 2 3  3.输出 输入 N 行，每行输出一个匹配的结果，格式为：\nm：编号 w：编号  例如：\nThis is a block. This is more of a block.  4.核心思想 4.算法实现 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; void input(); void process(); int findfman(); int findpreorder(int index, int o); void output(); void desource(); int N; int * mancp; int * womencp; int ** man; int ** women; int main(int argc, char *argv[]) { //freopen(\u0026#34;input.txt\u0026#34;,\u0026#34;r\u0026#34;,stdin); //freopen(\u0026#34;output.txt\u0026#34;,\u0026#34;w\u0026#34;, stdout);  scanf(\u0026#34;%d\u0026#34;, \u0026amp;N); input(); process(); output(); desource(); //fclose(stdin); //fclose(stdout);  return 0; } void input() { int i, j, k; man = (int **)malloc(sizeof(int *) * N); women = (int **)malloc(sizeof(int *) * N); mancp = (int *)malloc(sizeof(int) * N); womencp = (int *)malloc(sizeof(int) * N); for(i=0; i\u0026lt;N; ++i) { man[i] = (int *)malloc(sizeof(int) * N); women[i] = (int *)malloc(sizeof(int) * N); mancp[i] = 0; womencp[i] = 0; } for(i=0; i\u0026lt;N; ++i){ for(j=0; j\u0026lt;N; ++j){ scanf(\u0026#34;%d\u0026#34;, \u0026amp;k); man[i][k-1] = j+1; //按照喜好优先级 顺序输入。第(i+1)喜欢的是第K个女生。k是女生编号，对应的下标应减一。 \t} } for(i=0; i\u0026lt;N; ++i){ for(j=0; j\u0026lt;N; ++j) { scanf(\u0026#34;%d\u0026#34;, \u0026amp;k); women[i][k-1] = j+1; } } } void process() { int i, j, k; int count = N; int o; while(count) { o = 1; i = findfman(N); j = findpreorder(i, o); while( womencp[j] != 0 \u0026amp;\u0026amp; women[j][i] \u0026gt; women[j][womencp[j]-1]) { j = findpreorder(i, ++o); } if (womencp[j] == 0) { mancp[i] = j + 1; womencp[j] = i+1; count--; } else if(women[j][i] \u0026lt; women[j][womencp[j]-1]){ mancp[womencp[j] -1] = 0; mancp[i] = j + 1; womencp[j] = i+1; } } } //找到未分配的男性成员下标 int findfman() { int i; for(i=0; i\u0026lt;N; ++i) { if (mancp[i] == 0) return i; } } //根据男性index下标找到第o个中意的女性 int findpreorder(int index, int o) { int i; for(i=0; i\u0026lt;N; ++i) { if (man[index][i] == o) return i; } } void output() { int i; for(i=0; i\u0026lt;N; ++i) { printf(\u0026#34;m:%d w:%d\\n\u0026#34;,i+1, mancp[i]); } } void desource() { int i; for (i=0; i\u0026lt;0; ++i) { free(man[i]); free(women[i]); } free(man); free(women); free(mancp); free(womencp); } 5.测试数据  第一组： 输入：\n3 1 2 3 2 1 3 1 2 3 2 1 3 1 2 3 1 2 3  输出：\nm:1 w:1 m:2 w:2 m:3 w:3   第二组： 输入：\n10 2 5 4 7 3 6 9 8 10 1 3 2 1 4 5 9 6 10 8 7 5 4 9 10 1 3 2 8 7 6 3 8 7 2 5 4 9 1 10 6 6 4 9 5 3 1 8 7 2 10 6 1 2 8 3 9 10 5 7 4 7 3 10 9 1 8 6 2 5 4 10 8 2 9 1 3 6 7 4 5 9 1 6 10 7 8 2 3 5 4 8 10 2 7 3 1 6 9 5 4 4 6 1 7 10 5 9 8 3 2 1 2 4 3 8 9 6 10 5 7 2 1 8 10 4 3 6 9 7 5 3 1 7 4 5 2 6 8 10 9 6 7 9 4 3 1 10 5 2 8 4 1 2 8 10 9 3 5 7 6 7 3 8 9 1 10 5 2 6 4 10 4 2 3 1 8 6 7 9 5 5 2 6 10 7 8 1 3 9 4 7 1 2 8 3 10 6 9 5 4  输出：\nm:1 w:2 m:2 w:3 m:3 w:4 m:4 w:5 m:5 w:6 m:6 w:1 m:7 w:7 m:8 w:10 m:9 w:9 m:10 w:8   ","date":"2020-09-16T15:20:00+08:00","permalink":"https://cyasts.github.io/p/stablematche/","title":"稳定匹配问题"},{"content":"原文  壬戌之秋，七月既望，蘇子與客泛舟遊於赤壁之下。清風徐來，水波不興。舉酒屬客，誦明月之詩，歌窈窕之章。少焉，月出於東山之上，徘徊於斗牛之間。白露橫江，水光接天。縱一葦之所如，凌萬頃之茫然。浩浩乎如馮虛御風，而不知其所止；飄飄乎如遺世獨立,羽化而登仙。\n於是飲酒樂甚，扣舷而歌之。歌曰：“桂棹兮蘭槳，擊空明兮溯流光。渺渺兮予懷，望美人兮天一方。”客有吹洞簫者，倚歌而和之。其聲嗚嗚然，如怨如慕，如泣如訴；餘音嫋嫋，不絕如縷。舞幽壑之潛蛟，泣孤舟之嫠婦。\n蘇子愀然，正襟危坐，而問客曰：“何爲其然也？”客曰：“‘月明星稀，烏鵲南飛。’此非曹孟德之詩乎？西望夏口，東望武昌，山川相繆，鬱乎蒼蒼，此非孟德之困於周郎者乎？方其破荊州，下江陵，順流而東也，舳艫千里，旌旗蔽空，釃酒臨江，橫槊賦詩，固一世之雄也，而今安在哉？況吾與子漁樵於江渚之上，侶魚蝦而友麋鹿，駕一葉之扁舟，舉匏樽以相屬。寄蜉蝣於天地，渺滄海之一粟。哀吾生之須臾，羨長 江之無窮。挾飛仙以遨遊，抱明月而長終。知不可乎驟得，託遺響於悲風。”\n蘇子曰：“客亦知夫水與月乎？逝者如斯，而未嘗往也；盈虛者如彼，而卒莫消長也。蓋將自其變者而觀之，則天地曾不能以一瞬；自其不變者而觀之，則物與我皆無盡也，而又何羨乎！且夫天地之間，物各有主，苟非吾之所有，雖一毫而莫取。惟江上之清風，與山間之明月，耳得之而爲聲，目遇之而成色，取之無禁，用之不竭。是造物者之無盡藏也，而吾與子之所共適。”(共適 一作：共食) 客喜而笑，洗盞更酌。餚核既盡，杯盤狼籍。相與枕藉乎舟中，不知東方之既白。\n","date":"2020-09-14T21:23:00+08:00","permalink":"https://cyasts.github.io/p/life/","title":"赤壁赋"}]